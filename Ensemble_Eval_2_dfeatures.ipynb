{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "import gc\n",
    "import numpy as np, pandas as pd\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "\n",
    "os.chdir('D:\\\\kaggle_competitions\\\\M5 Competition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
    "         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n",
    "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50\n",
    "h = 28 \n",
    "max_lags = 57\n",
    "tr_last = 1913 ##changes for evaluation to 1945\n",
    "fday = datetime(2016,4, 25) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(is_train = True, nrows = None, first_day = 1200):\n",
    "    prices = pd.read_csv(\"sell_prices.csv\", dtype = PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "            \n",
    "    cal = pd.read_csv(\"calendar.csv\", dtype = CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "    \n",
    "    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n",
    "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol:\"float32\" for numcol in numcols} \n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    dt = pd.read_csv(\"sales_train_validation.csv\", \n",
    "                     nrows = nrows, usecols = catcols + numcols, dtype = dtype) ##changes the names\n",
    "    \n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "    \n",
    "    if not is_train:\n",
    "        for day in range(tr_last+1, tr_last+ 28 +1):\n",
    "            dt[f\"d_{day}\"] = np.nan\n",
    "    \n",
    "    dt = pd.melt(dt,\n",
    "                  id_vars = catcols,\n",
    "                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                  var_name = \"d\",\n",
    "                  value_name = \"sales\")\n",
    "    \n",
    "    dt = dt.merge(cal, on= \"d\", copy = False)\n",
    "    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    dt['var']=dt['d'].str.replace('d_','').astype(int)\n",
    "    #dt['var2']=dt['var']**2\n",
    "    #dt=dt[dt['var']>150]\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fea(dt):\n",
    "    lags = [7,28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    wins = [7,28]\n",
    "    for win in wins :\n",
    "        for lag,lag_col in zip(lags, lag_cols):\n",
    "            \n",
    "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    date_features = {\n",
    "        \n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "FIRST_DAY = 80 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk ! ##set to 50\n",
    "\n",
    "df = create_dt(is_train=True, first_day= FIRST_DAY)\n",
    "df.shape\n",
    "\n",
    "create_fea(df)\n",
    "df.shape\n",
    "\n",
    "#df=df[df['var']>150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=reduce_mem_usage(df)\n",
    "df.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id']+[\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\", \"wm_yr_wk\", \"weekday\"]\n",
    "train_cols = df.columns[~df.columns.isin(useless_cols)]\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_valid_inds=X_train.index.values[X_train['d'].str.contains('d_1885|d_1886|d_1887|d_1888|d_1889|d_1890|d_1891|d_1892|d_1893|d_1894|d_1895|d_1896|d_1897|d_1898|d_1899|d_1900|d_1901|d_1902|d_1903|d_1904|d_1905|d_1906|d_1907|d_1908|d_1909|d_1910|d_1911|d_1912|d_1913'),True]\n",
    "useless_cols = [\"id\", \"date\", \"sales\",'d', \"wm_yr_wk\", \"weekday\"]\n",
    "train_cols = df.columns[~df.columns.isin(useless_cols)]\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(786)\n",
    "\n",
    "train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
    "train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
    "                         categorical_feature=cat_feats, free_raw_data=False)\n",
    "fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n",
    "                              categorical_feature=cat_feats,\n",
    "                 free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'poisson', ###try this with tweedie ##and tweedie reult in 0.493\n",
    "                    #'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.04,#riginal=0.0409 ###i'll try this with the same lr a\n",
    "                    'num_leaves': 2**14,\n",
    "                    'min_data_in_leaf': 2**15,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100, ###100\n",
    "                    'n_estimators': 1800,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': 1,\n",
    "                } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lgb.save_model(\"with_onlyd_features_int_poisson.lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lgb.plot_importance(m_lgb)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "alphas=[1]\n",
    "weights = [1/len(alphas)]*len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "    te = create_dt(False)\n",
    "    cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "    for tdelta in range(0, 28):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(tdelta, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "        create_fea(tst)\n",
    "        tst = tst.loc[tst.date == day , train_cols]\n",
    "        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
    "\n",
    "\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "    te_sub.fillna(0., inplace = True)\n",
    "    te_sub.sort_values(\"id\", inplace = True)\n",
    "    te_sub.reset_index(drop=True, inplace = True)\n",
    "    #te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
    "    if icount == 0 :\n",
    "        sub = te_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += te_sub[cols]*weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "\n",
    "sub2 = sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission_final_val_poisson_changes.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
